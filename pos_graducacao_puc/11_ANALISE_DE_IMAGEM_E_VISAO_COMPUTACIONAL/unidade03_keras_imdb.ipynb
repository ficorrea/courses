{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unidade03_keras_imdb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G26TeWKT5yRV"
      },
      "source": [
        "Neste exemplo você irá construir um classificador binário para o dataset do IMDB (NLP Problem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uivPCP-y7OX_"
      },
      "source": [
        "O imdb dataset já vem com o Keras e já é pré-processado. Os reviews (sequência de palavras) já estão organizados em sequências de inteiros, em que cada inteiro significa uma palavra específica no dicionário.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaiSFoNw7Tap"
      },
      "source": [
        "from keras.datasets import imdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ-PM9un7aoV"
      },
      "source": [
        "As 10000 palavras mais frequentes serão mantidas no dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zIEOFZD7WzS"
      },
      "source": [
        "# call load_data with allow_pickle implicitly set to true\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Czjg_p-7h8L"
      },
      "source": [
        "Train_data e test_data são as listas de reviews, cada review é uma lista de índices de palavras (texto do review codificado no índice de palavras)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE4gMouj7iyg"
      },
      "source": [
        "print(train_data[6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4kimfA97pUD"
      },
      "source": [
        "Train_labels e test_labels são as listas de 0’s e 1’s (0 = negativo review e 1 = positivo review)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxiSXNaA7qAO"
      },
      "source": [
        "print(train_labels[6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph-hiD5b7uzh"
      },
      "source": [
        "Veja que nenhum índice de palavra irá exceder 10000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd_Sqcyq7v1K"
      },
      "source": [
        "max([max(sequence) for sequence in train_data])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Y5vzYmbZyu"
      },
      "source": [
        "Não podemos inserir um lista de inteiros na rede. Temos que converter a listar para um tensor no formato (samples, word_indices)\n",
        "\n",
        "Por exemplo, transformar a sequência [3, 5] em um vetor de 10.000 dimensões que seria todos os 0s, exceto os índices 3 e 5, que seriam 1s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rElazRb727e"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3krBFRu174zb"
      },
      "source": [
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  # Creates an all-zero matrix of shape (len(sequences), dimension)\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    # Sets specific indices of results[i] to 1s\n",
        "    results[i, sequence] = 1.\n",
        "\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgHczT5U7_jO"
      },
      "source": [
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKfRKRbae1e5"
      },
      "source": [
        "print(x_train[0])\n",
        "print(x_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9HlCFIy8Ek7"
      },
      "source": [
        "O mesmo para os vetores de labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msXzaOO18FN7"
      },
      "source": [
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7VQBVAZxjy2"
      },
      "source": [
        "**Construindo a rede**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYy49mi88gAi"
      },
      "source": [
        "O input é um vetor e os labels são escalares (1’s e 0’s). Vamos usar um fully connected (Dense) com relu como activation function.\n",
        "\n",
        "Dense(16, activation='relu’)\n",
        "\n",
        "16: é o argumento para cada dense layer. Ou seja, é o número de hidden units da camada (é a dimensão da representação da camada). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzAKlWUE8lqs"
      },
      "source": [
        "**Configurando a rede**\n",
        "\n",
        "Iremos configurar a rede com duas camadas intermediárias com 16 unidades ocultas cada.\n",
        "Uma terceira camada que produzirá a previsão escalar em relação ao sentimento do review em questão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0G2rqdM8oAA"
      },
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q25E50xQ8tjE"
      },
      "source": [
        "Escolhendo a loss function e o optimizer (como strings por já fazerem parte do Keras)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFeKYO458vO8"
      },
      "source": [
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics\n",
        "# from keras.optimizers import SGD\n",
        "\n",
        "# model.compile(optimizer=RMSprop(lr=0.001),\n",
        "# loss=losses.binary_crossentropy,\n",
        "# metrics=[metrics.binary_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDUn6frW88aV"
      },
      "source": [
        "No entanto, você pode deixar tudo como padrão:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruuflKQK89Xl"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "loss='binary_crossentropy',\n",
        "metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etCwaa9I82e2"
      },
      "source": [
        "Para monitorar o treinamento, criamos o conjunto de validação separando 10000 amostras do conjunto original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD7ryltp83qq"
      },
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zIqCQ7Q9DaU"
      },
      "source": [
        "Iremos treinar o modelo por 20 épocas (20 iterações sobre todas as amostras do treinamento) em um batch de 512 amostras.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jKr6P9R9EPu"
      },
      "source": [
        "model.fit(partial_x_train,\n",
        "partial_y_train,\n",
        "epochs=100,\n",
        "batch_size=512,\n",
        "validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAIx01De2iEH"
      },
      "source": [
        "**Usando uma rede treinada para gerar previsões sobre novos dados**\n",
        "\n",
        "Gerando a probabilidade de cada análise ser positiva com o método predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AblWlU902Vwi"
      },
      "source": [
        "print(model.predict(x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfh7Wnke2yCd"
      },
      "source": [
        "A rede está confiante para algumas amostras (0,99 ou mais, ou 0,01 ou menos), mas menos confiante para outras (0,6, 0,4)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXj1c-Qc9klv"
      },
      "source": [
        "**Treinando o modelo do zero**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxI-sMJT8XI3"
      },
      "source": [
        "Observe que no treino abaixo não separamos o conjunto de treino em treino e validação, uma vez que não iremos validar o modelo durante o treinamento e sim após o mesmo com o model.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQyrpyMDakZX"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvWpyiCEal0f"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "loss='binary_crossentropy',\n",
        "metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaQ7Kijk9myU"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "results = model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "929EVdBp72US"
      },
      "source": [
        "results = model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oNmOQZIrW38"
      },
      "source": [
        "Referência: François Chollet. Deep Learning with Python. November 2017  "
      ]
    }
  ]
}