{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Unidade 2.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"collapsed":true,"id":"TfMtJgRJAC2s"},"source":["!pip install nltk==3.5\n","!pip install gensim\n","!pip install umap-learn\n","!pip install wikipedia\n","!pip install unidecode"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6V8u14WAC26"},"source":["import re\n","import nltk\n","from nltk.util import ngrams\n","from nltk.corpus import stopwords\n","import wikipedia\n","import string\n","from unidecode import unidecode\n","from nltk.tokenize import sent_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","import urllib.request\n","import bz2\n","import gensim\n","import warnings\n","import numpy as np\n","from gensim.models import word2vec\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","import umap\n","warnings.filterwarnings('ignore')\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X2ZMw4zfAC3A"},"source":["# Definição do Corpus"]},{"cell_type":"markdown","metadata":{"id":"Fumti3IlAC3B"},"source":["## Base"]},{"cell_type":"code","metadata":{"id":"JhMedbmMAC3C"},"source":["wikipedia.set_lang(\"pt\")\n","bh = wikipedia.page(\"Belo Horizonte\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eJ1WHOADAC3G"},"source":["corpus = bh.content"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TxnmUk2NAC3M"},"source":["Selecionamos algumas frases do corpus de BH da wikipedia.\n","\n","Conside a lista abaixo como nosso corpus de documentos. Cada elemento da lista, considere como um único documento."]},{"cell_type":"code","metadata":{"id":"Pd4pTiK0AC3M"},"source":["documentos = \\\n","[\"Belo Horizonte é um município brasileiro e a capital do estado de Minas Gerais\",\n","\"A populacao de Belo Horizonte é estimada em 2 501 576 habitantes, conforme estimativas do Instituto Brasileiro de Geografia e Estatística\",\n","\"Belo Horizonte já foi indicada pelo Population Crisis Commitee, da ONU, como a metrópole com melhor qualidade de vida na América Latina\",\n","\"Belo Horizonte é mundialmente conhecida e exerce significativa influência nacional e até internacional, seja do ponto de vista cultural, econômico ou político\",\n","\"Belo Horizonte é a capital do segundo estado mais populoso do Brasil, Minas Gerais\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G1vjVVoYAC3T"},"source":["## Preprocessamento"]},{"cell_type":"markdown","metadata":{"id":"V_MuzVllAC3U"},"source":["<b> Atividade </b>\n","\n","1) Escreva uma método que realiza o pré-processamento da lista de <b>documentos</b>.\n","\n","O método deve, para cada documento:\n","- tokenizar cada palavra\n","- remover stopwords\n","- remover números\n","- remover pontuções\n","- remover acentos"]},{"cell_type":"code","metadata":{"id":"tsfgYeVIa9PX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O9fbyAieAC3j"},"source":["# Representação Textual"]},{"cell_type":"markdown","metadata":{"id":"Bxy--9uzAC3x"},"source":["### Phrases - Gensim\n","\n","Forma mais inteligente de calcular os bigrams. Ela calcula os bigramas levando em consideração a frequência do par das palavaras em todos os documentos.\n","Para isso ele treina um modelo e depois aplica no corpus.\n","\n","```python\n","#treinamento bigrams\n","model_corpus_phrases = gensim.models.Phrases(corpus_processado, min_count=1)\n","#calulando os bigrams do corpus processado\n","bigram_corpus = model_corpus_phrases[corpus_processado]\n","```\n","\n","<b> Atividade </b>\n","\n","2) Faça um código que treine os bigrams, sendo que o <b>min_count = 1</b>. \n","O <b>min_count</b> é a contagem mínima que aquele par de palavras deve aparecer junto para considerarmos com um token. Teste também com outros valores de mim_count. Depois imprima os bigramas de cada documento.\n","Use o corpus_processado."]},{"cell_type":"code","metadata":{"id":"JArQt0OPa_8J"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jFHut-GWAC4L"},"source":["## TD-IDF\n","\n","Dica de leitura: https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XklQxnVKj7c\n","\n","Para representar o texto com TF-IDF utilizamos o TfidfVectorizer. A seguir apresentamos instruções sobre como utilizá-lo.\n","\n","```python\n","#primeiro criamos o objeto\n","vect = TfidfVectorizer()\n","vect #aqui você pode observa todos os parâmetros que o objeto possui\n","## Existem alguns parâmetros, opcionais, que podemos informar para uma possível melhora do nosso modelo. Por exemplo:\n","### inclui 1-grams e 2-grams\n","vect.set_params(ngram_range=(1, 2))\n","### ignora termos que a aparecem em mais de 50% dos documentoss\n","vect.set_params(max_df=0.5)\n","### só considero termos que aparecem em ao menos 2 documentos\n","vect.set_params(min_df=2)\n","\n","#depois aplicamos fit_transform para transformar o texto em números\n","docs_tdidf = vect.fit_transform(docs)\n","\n","#o docs_tdidf é uma matriz com os números que representam cada um dos textos. \n","## Conseguimos verificar a dimensão desta matriz:\n","print(docs_tdidf.shape)\n","\n","#Para visualizar as features capturadas pelo TF-IDF utilize:\n","print(vect.get_feature_names())\n","#Para visualizar os vetores correspondentes a cada palavara utilize:\n","df = pd.DataFrame(docs_tdidf.T.todense(), index=vect.get_feature_names(), columns=[\"doc\"+str(i+1) for i in range(0,len(docs))])\n","df\n","```"]},{"cell_type":"markdown","metadata":{"id":"jZD0n75gAC4M"},"source":["<b> Atividade: </b>\n","\n","3) Faça o TDIFTVectorizer nos documentos da variável <b>documentos</b> sem alterar nenhum parâmetro. "]},{"cell_type":"code","metadata":{"id":"d6WMh8XBbClS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AbTCuJzIAC4o"},"source":["<b> Atividade </b>\n","\n","4) Imprima o shape do resultado da atividade 4"]},{"cell_type":"code","metadata":{"id":"mOL6Ajq9bD_C"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vQfYws4lAC4t"},"source":["<b> Atividade </b>\n","\n","5) Imprima as features capturadas em 4."]},{"cell_type":"code","metadata":{"id":"eDrMtITzbEw2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6kcdd6_IAC4x"},"source":["6) Imprima os vetores correspondentes a cada palavra de cada documento."]},{"cell_type":"code","metadata":{"id":"Oa9LfGTsbLoO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FF-ZevNeAC4-"},"source":["## Bag of Words\n","\n","Para representar o bag of words utilizamos o CountVectorizer\n","\n","```python\n","#primeiro criamos o objeto\n","vect_bag = CountVectorizer(binary=True) #se binary = False -> ocorre a contagem da frequência em que a palavra aparece\n","vect_bag #imprime os parâmetros\n","\n","```\n","\n","<b> Atividade </b>\n","\n","7) Faça o CountVectorizer nos documentos da variável <b>documentos</b> considerando binary = True"]},{"cell_type":"code","metadata":{"id":"2z8_t3vabNMi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z0ZNTXLWAC5I"},"source":["<b> Atividade </b>\n","\n","8) Imprima o índice correspondente a cada token da lista retornada por vect_bag.get_feature_names()"]},{"cell_type":"code","metadata":{"id":"2t8GJUMKbQF7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LbLt5-LiAC5v"},"source":["## Embedding"]},{"cell_type":"markdown","metadata":{"id":"3nmucJX2AC5v"},"source":["### Utilizando um embedding treinado"]},{"cell_type":"markdown","metadata":{"id":"cf94ykmVAC5w"},"source":["<b> Atividade </b>\n","\n","9) Faça download do seguinte arquivo, realize a leitura deste arquivo e carregue o modelo: \n","https://drive.google.com/file/d/1f5sNZcV8LDam4zxbHnkm472r3r8D_UpX/view?usp=sharing\n","\n","Depois suba no seu drive e carregue,\n","\n","obs.: como o embedding é pesado, demora um pouco para carregar, cerca de 4 minutos."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4VepWYTrPt65","executionInfo":{"status":"ok","timestamp":1622748703494,"user_tz":180,"elapsed":33468,"user":{"displayName":"Bárbara Silveira Fraga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSYKhKST3-JxDhaPKtydW_1gnhsEFC_X_IjaV=s64","userId":"08155615283259294201"}},"outputId":"7fb09f39-4cf5-4997-dcf5-31d857766fb6"},"source":["#opção 1 -> montar o drive no colab e acessar o arquivo de embedding do drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#opção 2 -> fazer download e fazer upload por aqui\n","#from google.colab import files\n","#uploaded = files.upload()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S5thOv6AAC50","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622749239253,"user_tz":180,"elapsed":208139,"user":{"displayName":"Bárbara Silveira Fraga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSYKhKST3-JxDhaPKtydW_1gnhsEFC_X_IjaV=s64","userId":"08155615283259294201"}},"outputId":"759ce1f0-dec1-4215-dd2a-84c2fb36e75d"},"source":["%%time\n","# carregar\n","##na variável path coloque o caminho do embedding baixado:\n","path = \"/content/drive/MyDrive/PUC/Pós Graduação/EAD | 2021 | NLP/Aulas/Unidade 2/Prática/embeddings/ptwiki_20180420_100d.txt.bz2\"\n","word_vectors = gensim.models.KeyedVectors.load_word2vec_format(path, binary=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 3min 26s, sys: 1.54 s, total: 3min 28s\n","Wall time: 3min 27s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y7OvGRFIAC55"},"source":["<b> Atividade </b>\n","\n","10) Imprima os vetores das palavras \"nlp\" e \"computacao\"\n","\n","```python\n","#exemplo de retorno do vetor\n","word_vectors[__]\n","```"]},{"cell_type":"code","metadata":{"id":"c_UyaYIUAC59"},"source":["word_vectors.vocab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uWL_yba-bXIh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rkeSO-BJAC6B"},"source":["<b> Similaridade de Vetores </b> \n","\n","No gensim é possível realizar a similaridade utilizando o seguinte método:\n","\n","```python\n","word_vectors.most_similar(___)\n","```\n","\n","<b> Atividade </b>\n","\n","11) Verifique a similaridade das seguintes palavras: elizabete, raiva, segunda, dois, computação."]},{"cell_type":"code","metadata":{"id":"Fzy1VhXcbcee"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Tc5XIJIAC6W"},"source":["<b> Operação Vetorial</b>\n","\n","Na aula teórica, estudamos sobre as operação entre os vetores. \n","Agora vamos ver na prática:\n","\n","```python\n","#exemplo:\n","word_vectors.wv.most_similar(positive=['mulher', 'rei'], negative=['homem'], topn=10)\n","```\n","\n","<b>Atividade</b>\n","\n","12) Execute o exemplo acima em uma célula e repita para os seguintes cenários:\n","\n","- menino, menina, homem\n","- caminhada, andar, correr\n","- filho, filha, irmã\n","- pai, mãe, avô\n","\n","<b> Reflita </b> as palavras similares fazem sentido?"]},{"cell_type":"code","metadata":{"id":"Upx2n9h9bhKm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bEGWYtn4AC7P"},"source":["### Treinando seu embedding\n","\n","Aqui vamos utilizar o corpus machado. São textos/contos escritos por Machado de Assis.\n","Esse corpus é diponibilizado pelo NLTK."]},{"cell_type":"code","metadata":{"id":"Rdmp3F3OTl0B"},"source":["from nltk.corpus import machado"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDebqiPkLE3h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622748104922,"user_tz":180,"elapsed":228,"user":{"displayName":"Bárbara Silveira Fraga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSYKhKST3-JxDhaPKtydW_1gnhsEFC_X_IjaV=s64","userId":"08155615283259294201"}},"outputId":"f3aa4ce4-913a-4aaa-e497-666284f9c9c4"},"source":["import nltk\n","from nltk.corpus import machado\n","nltk.download('machado')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package machado to /root/nltk_data...\n","[nltk_data]   Package machado is already up-to-date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"EXFpVSHcRACL"},"source":["raw_casmurro = machado.raw('contos/macn001.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XHribhOLAC7m"},"source":["O método ''machado_sents()'' retorna todo o texto quebrado pelas setenças e já tokenizado.\n","\n","As sentenças são separadas pelo \"\\n\". Dentro de cada sentença, divide os tokens separadas pelo espaço."]},{"cell_type":"code","metadata":{"id":"r15rrg-qAC7m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622748109751,"user_tz":180,"elapsed":370,"user":{"displayName":"Bárbara Silveira Fraga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSYKhKST3-JxDhaPKtydW_1gnhsEFC_X_IjaV=s64","userId":"08155615283259294201"}},"outputId":"7c6f6c06-dcc2-4263-dc84-eef119c63e50"},"source":["machado_sents = machado.sents()\n","print(machado_sents)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[['Conto', ',', 'Contos', 'Fluminenses', ',', '1870'], ['Contos', 'Fluminenses'], ...]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ek1UIrtOAC7s"},"source":["Vamos relizar um pré-processamento mínimo nos dados. Lembrando que: o pré-processamento é impotatíssimo no resultado final.\n","\n","<b> Atividade </b>\n","\n","13) Aplique as técnicas abaixo no documento <b> machado_sents</b>: \n","\n","- lower\n","- remoção pontuações"]},{"cell_type":"code","metadata":{"id":"m_-j4YppbonC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RhJKgQwTAC72"},"source":["<b> Treinando o embedding </b> \n","\n","Para treinar os embeddings existem alguns parâmetros, vide exemplo abaixo:\n","\n","```python\n","#Alguns parâmetros:\n","## size -> dimensão vetor\n","## min_count -> ignora todas palavras cuja frequência mínima é menor que este\n","## workers -> quantas threads serão utilizadas para treinar o modelo\n","## seed -> seed para geração do numero aleatório. \n","## sg -> 1 para skip-gram; caso contrário CBOW.\n","## window -> contexto, Distância máxima entre a palavra atual e a prevista em uma frase. O default é 5.\n","model = word2vec.Word2Vec(text_preproc, min_count=10, workers=4, seed=123, sg=1, size=300, window=5)\n","```\n","\n","<b> Atividade </b>\n","\n","14) Gere os embeddings com o texto processado do documento de Machado de Assis.\n"]},{"cell_type":"code","metadata":{"id":"-M_AZkXtbrM5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tvhul62-AC79"},"source":["\n","<b> Atividade </b>\n","\n","15) Faça os itens abaixo:\n","\n","- Verifique o vetor de embeddings da variável \"dom\"\n","- Verifique a similaridade entre \"mulher\" e \"homem\"\n","- Verifique a similaridade entre \"dom\" e \"casmurro\""]},{"cell_type":"code","metadata":{"id":"TZonLVIcbuaF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ic9DbG1rbuhM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gzTUg-IaAC8K"},"source":["<b> Salvando o modelo</b>"]},{"cell_type":"code","metadata":{"id":"VB9agjQYAC8L"},"source":["model.wv.save_word2vec_format('model_TESTE.bin', binary=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xMjXdQ13AC8P"},"source":["<b> Atividade </b>\n","\n","Dada as seguintes palavras:\n","\n","foi, relógio, amor, raiva, brasil.\n","\n","16) Escreva um método que retorne uma lista com as 5 palavras similares de cada uma das listadas anteriormente.\n","Imprima a lista das palavras similares, incluindo a palavra origem."]},{"cell_type":"code","metadata":{"id":"e5d6DypjbyE9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sX5yNDdNAC8n"},"source":["### Visualização"]},{"cell_type":"markdown","metadata":{"id":"q_8sL0sOAC8o"},"source":["Para a visualização dos embeddings iremos  construir um array com todas as palavras retornadas anteriormente.\n","\n","<b> Atividade </b>\n","\n","17) Primeiro, gere uma única lista com todas as palavras retornadas anteriomente. O array deve ter tamanho 30."]},{"cell_type":"code","metadata":{"id":"V2mnmx59AC8o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IhO4tuvyAC8w"},"source":["18) O código abaixo gera um array com todos os embeddings das palavras anteriores. Este array terá dimensão (30,300)"]},{"cell_type":"code","metadata":{"id":"2T4mDJ_cAC8x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622748293782,"user_tz":180,"elapsed":248,"user":{"displayName":"Bárbara Silveira Fraga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSYKhKST3-JxDhaPKtydW_1gnhsEFC_X_IjaV=s64","userId":"08155615283259294201"}},"outputId":"5c3c1d6f-02bd-42a9-9125-37b05e4bb1f1"},"source":["array_embeddings = np.empty((0,300), dtype='f')\n","for w in all_words:\n","    array_embeddings = np.append(array_embeddings, np.array([model[w]]), axis=0)\n","print(array_embeddings.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(30, 300)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2YjvhwhEAC85"},"source":["<b>Dica: </b> Use a função abaixo para plotar o array 2D que será gerado com o método PCA, TSNE e UMAP"]},{"cell_type":"code","metadata":{"id":"stqEUCMSAC86"},"source":["def plot_embedding_2d(array_2d, all_words, words_seed):\n","    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n","    for (x, y), w in zip(array_2d, all_words):\n","        ax.scatter(x, y, c='red' if w in words_seed else 'blue')\n","        ax.annotate(w,\n","                     xy=(x, y),\n","                     xytext=(5, 2),\n","                     textcoords='offset points',\n","                     ha='right',\n","                     va='bottom')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dPvF4mqEAC89"},"source":["#### PCA\n","\n","<b> É uma ténica que existe a mais de século. É rápido, determinístico e linear. Essa linearidade limita sua utilidade em domínios complexos, como linguagem natural ou imagens, onde a estrutura não linear. </b>\n","\n","Mais informações: https://medium.com/towards-artificial-intelligence/machine-learning-dimensionality-reduction-via-principal-component-analysis-1bdc77462831\n","\n","\n","<b> Atividade </b>\n","\n","19) Gere a visualização dos embeddings anteriores utilizando o PCA para reduzir a dimensionalidade.\n","\n","Exemplo do PCA:\n","\n","```python\n","#uso de PCA\n","pca = PCA(n_components=2)\n","pca_result = pca.fit_transform(array_embeddings)\n","```"]},{"cell_type":"code","metadata":{"id":"Q_cir8Hjb97F"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DiydMeXhAC9G"},"source":["#### TSNE\n","<b> Uma técnica mais recente que captura estrutura não linear é o t-SNE, que significa distribuição estocástica de embedding viziznhos em t ( t-distributed Stochastic Neighbor Embedding). \n","É uma ótima técnica para capturar a estrutura não linear em dados de alta dimensão(pelo menos em nível local). Isto é, dois pontos que são próximos no espaço de alta dimensão a probabilidade de estarem próximos em uma dimensão baixa é alta. </b>\n","\n","Mais informações: https://medium.com/@garora039/dimensionality-reduction-using-t-sne-effectively-cabb2cd519b\n","\n","<b> Atividade </b>\n","\n","20) Gere a visualização dos embeddings anteriores utilizando o TSNE para reduzir a dimensionalidade.\n","\n","Exemplo do TSNE:\n","\n","```python\n","#uso de TSNE\n","tsne = TSNE(n_components=2, random_state=0, perplexity=4)\n","tsne_result =  tsne.fit_transform(array_embeddings)\n","```"]},{"cell_type":"code","metadata":{"id":"V4AMZ2UZcDQa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5mpCJ_VjAC9P"},"source":["#### UMAP\n","\n","<b> Técnica super nova!! Foi lançada em 2018! Observe que o resultado do UMAP com o TSNE é semelhante. Mas existem várias vantagens do UMAP, por exemplo: é mais rápido que o t-SNE; ele captura melhor a estrutura global </b>\n","\n","Mais informações: https://medium.com/@dan.allison/dimensionality-reduction-with-umap-b081837354dd\n","\n","<b> Atividade </b>\n","\n","21) Gere a visualização dos embeddings anteriores utilizando o UMAP para reduzir a dimensionalidade.\n","\n","Exemplo do UMAP:\n","\n","```python\n","#uso de TSNE\n","umap = umap.UMAP()\n","umap_result =  umap.fit_transform(array_embeddings)\n","```"]},{"cell_type":"code","metadata":{"id":"X5L1MYnGcGHl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANms6_aWbG6D"},"source":[""],"execution_count":null,"outputs":[]}]}