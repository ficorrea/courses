{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics & Experimental Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we will implement the main metrics used to evaluate recommender systems.\n",
    "We also will realize specific analyzes about the RS implemented in the last classes.\n",
    "This kind of experimentation is similar to what is expected from you in the final project.\n",
    "For this reason, you must do:\n",
    "\n",
    "- Read the train and test files\n",
    "- Implement the main evaluation metrics\n",
    "- Plot graphics to analyze the RSs' behavior\n",
    "- Answer few questions about them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "import operator\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import OrderedDict\n",
    "\n",
    "# useful command\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading train and test files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read this file as you prefer. I propose to read the files by the pandas' library and create the sparse matrix after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = \n",
    "df_test = \n",
    "\n",
    "df_train.head()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sparse Matrix for these sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I propose to use the csr_matrix from scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select users, items and ratings logs (i.e., all information from each column)\n",
    "users = \n",
    "items = \n",
    "ratings = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the matrix dimensions based on the max index related to users and items\n",
    "nb_users = max(users)\n",
    "nb_items = max(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating matrix of ratings\n",
    "train_matrix = csr_matrix((ratings, (users, items)), shape=(nb_users+1, nb_items+1))\n",
    "\n",
    "train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select users, items and ratings logs from test (i.e., all information from each column)\n",
    "users = \n",
    "items = \n",
    "ratings = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating matrix of ratings\n",
    "test_matrix = csr_matrix((ratings, (users, items)), shape=(nb_users+1, nb_items+1))\n",
    "\n",
    "test_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to read the recommendations generated by each RS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readRecommendation(file_name):\n",
    "    \n",
    "    recommendations = {}\n",
    "    file_in = open(file_name, 'r')\n",
    "    \n",
    "    # for each line\n",
    "    for row in file_in:\n",
    "        row = row.rstrip()\n",
    "        # extracting userId\n",
    "        values = row.split(\"\\t\")\n",
    "        userId = int(values[0])\n",
    "        recommendations[userId] = []\n",
    "        # extracting items\n",
    "        values[1] = values[1].replace('[','')\n",
    "        values[1] = values[1].replace(']','')\n",
    "        recList = values[1].split(',')\n",
    "        # saving items\n",
    "        for tupla in recList:\n",
    "            v = tupla.split(':')\n",
    "            recommendations[userId].append(int(v[0]))\n",
    "    \n",
    "    file_in.close()\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading all files saved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_MostPopular = readRecommendation(\"recList_MostPopular.txt\")\n",
    "rec_BestRated = readRecommendation(\"recList_BestRated.txt\")\n",
    "rec_Rocchio = readRecommendation(\"recList_Rocchio.txt\")\n",
    "rec_PureSVD = readRecommendation(\"recList_PureSVD.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_MostPopular[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to implement:\n",
    "- Hit rate\n",
    "- Precision\n",
    "- Recall\n",
    "- MRR\n",
    "\n",
    "PS: You should define if one item is relevant or not based on the item's rating and the average users' ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_hitRate(users_targets, test_matrix, recommendation):\n",
    "    \n",
    "    hitRate = {}\n",
    "    \n",
    "    for u in users_targets:\n",
    "        # items consumed by u in test set\n",
    "        items_consumed = \n",
    "        # hits represented by intersection\n",
    "        hits = \n",
    "        # saving\n",
    "        hitRate[u] = len(hits)\n",
    "        \n",
    "    return hitRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_precision_recall(users_targets, test_matrix, recommendation):\n",
    "    \n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    \n",
    "    for u in users_targets:\n",
    "        # items consumed by u in test set\n",
    "        items_consumed = \n",
    "        # measuring u's average rating\n",
    "        mean_rating = \n",
    "        # selecting relevant items for u\n",
    "        relevants = []\n",
    "        for i in items_consumed:\n",
    "            if ( ):\n",
    "                relevants.append(i)\n",
    "        # relevants INT retrieved\n",
    "        inter = \n",
    "        # measuring precision\n",
    "        precision[u] = len(inter)/float( )\n",
    "        # measuring recall            \n",
    "        recall[u] = len(inter)/float( )\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_reciprocalRank(users_targets, test_matrix, recommendation):\n",
    "    \n",
    "    reciprocalRank = {}\n",
    "    \n",
    "    for u in users_targets:\n",
    "        # items consumed by u in test set\n",
    "        items_consumed = \n",
    "        # measuring u's average rating\n",
    "        mean_rating = \n",
    "        # selecting relevant items for u\n",
    "        relevants = []\n",
    "        for i in items_consumed:\n",
    "            if ( ):\n",
    "                relevants.append(i)\n",
    "        # measuring RR\n",
    "        reciprocalRank[u] = 0\n",
    "        cont = \n",
    "        for i in recommendation[u]:\n",
    "            if ( ):\n",
    "                reciprocalRank[u] = \n",
    "                break\n",
    "            else:\n",
    "                cont += 1\n",
    "                \n",
    "    return reciprocalRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing recommendation effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_targets = rec_MostPopular.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statisticInformation(distribution, name):\n",
    "    \n",
    "    print(\"RS:\", name)\n",
    "    print(\"\\t- Min:\", np.min(distribution))\n",
    "    print(\"\\t- Max:\", np.max(distribution))\n",
    "    print(\"\\t- Mean:\", np.mean(distribution))\n",
    "    print(\"\\t- Median:\", np.median(distribution))\n",
    "    print(\"\\t- STD:\", np.std(distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDistribution(distribution, x_label, y_label, name):\n",
    "    \n",
    "    distribution.sort(reverse=True)\n",
    "    plt.plot(distribution, color='blue')\n",
    "    plt.title(name)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring hit-rate and analyzing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitRate_MP = metric_hitRate(users_targets, test_matrix, rec_MostPopular)\n",
    "hitRate_BR = metric_hitRate(users_targets, test_matrix, rec_BestRated)\n",
    "hitRate_Roc = metric_hitRate(users_targets, test_matrix, rec_Rocchio)\n",
    "hitRate_SVD = metric_hitRate(users_targets, test_matrix, rec_PureSVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing min, max, average, median and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statisticInformation(list(hitRate_MP.values()), \"Most-Popular\")\n",
    "statisticInformation(list(hitRate_BR.values()), \"Best-Rated\")\n",
    "statisticInformation(list(hitRate_Roc.values()), \"Rocchio\")\n",
    "statisticInformation(list(hitRate_SVD.values()), \"PureSVD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some of these distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDistribution(list(hitRate_MP.values()), \"Users\", \"Hit Rate\", \"Most Popular\")\n",
    "#plotDistribution(list(hitRate_BR.values()), \"Users\", \"Hit Rate\", \"Best-Rated\")\n",
    "#plotDistribution(list(hitRate_Roc.values()), \"Users\", \"Hit Rate\", \"Rocchio\")\n",
    "plotDistribution(list(hitRate_SVD.values()), \"Users\", \"Hit Rate\", \"PureSVD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_MP, recall_MP = metric_precision_recall(users_targets, test_matrix, rec_MostPopular)\n",
    "precision_BR, recall_BR = metric_precision_recall(users_targets, test_matrix, rec_BestRated)\n",
    "precision_Roc, recall_Roc = metric_precision_recall(users_targets, test_matrix, rec_Rocchio)\n",
    "precision_SVD, recall_SVD = metric_precision_recall(users_targets, test_matrix, rec_PureSVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**: Analyzing min, max, average, median and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statisticInformation(list(precision_MP.values()), \"Most-Popular\")\n",
    "statisticInformation(list(precision_BR.values()), \"Best-Rated\")\n",
    "statisticInformation(list(precision_Roc.values()), \"Rocchio\")\n",
    "statisticInformation(list(precision_SVD.values()), \"PureSVD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall:** Analyzing min, max, average, median and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statisticInformation(list(recall_MP.values()), \"Most-Popular\")\n",
    "statisticInformation(list(recall_BR.values()), \"Best-Rated\")\n",
    "statisticInformation(list(recall_Roc.values()), \"Rocchio\")\n",
    "statisticInformation(list(recall_SVD.values()), \"PureSVD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer some questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questão 1:** Suponha que você foi contratado para uma empresa de e-commerce para utilizar propor um recomendador que maximize o número de vendas. Ele não está interessado em satisfazer ao cliente, mas apenas em vender. Qual dos recomendadores já implementados você sugere e por quê? Comprove com uma análise sua proposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questão 2:** Em uma última análise do sistema, foi constatado que a maioria dos usuários gastam muito tempo até comprar um produto (desconsiderando o tempo de compra e escolha). Você consegue imaginar o porquê disso? Como você poderia avaliar se o recomendador está contribuindo para esse problema? Qual dos propostos anteriormente você sugeriria? Mostre a análise realizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questão 3:** Em todo sistema existem diferentes grupos de perfis dos usuários. Um exemplo clássico são os usuários que preferem os produtos populares (i.e., que estão na moda) e os que não consomem itens populares. \n",
    "\n",
    "a) Em geral, para qual deles é mais fácil acertar as recomendações? Por quê?\n",
    "\n",
    "b) Se no seu sistema existem mais usuários com preferências por não populares, qual recomendador (dentre os implementados) você sugeriria? Mostre uma análise que comprove sua teoria.\n",
    "\n",
    "---------------\n",
    "Dica: Existem duas análises interessantes para esse problema. \n",
    "1. Medir métricas como novidade e diversidade dos itens recomendados, uma vez que elas tendem a mensurar o quão não-popular são os itens. Existem frameworks prontos com essas métricas. Aqui estão dois exemplos:\n",
    "    - https://github.com/RankSys/RankSys/wiki/Novelty-and-Diversity\n",
    "    - https://github.com/dcomp-labPi/FAiR \n",
    "2. É possível separar os usuários em três grupos: (1) os que tem no seu histórico mais 70% de itens populares; (2) os que possuem menos de 30% de itens populares consumidos; e (3) os que estão entre essas faixas de valores. Você pode fazer isso, simplesmente contando quantos itens de cada usuário estão no top-100 mais populares. Definido isso, basta re-avaliar os recomendadores sobre os usuários do grupo (2). Assim você conseguirá ver o desempenho deles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
