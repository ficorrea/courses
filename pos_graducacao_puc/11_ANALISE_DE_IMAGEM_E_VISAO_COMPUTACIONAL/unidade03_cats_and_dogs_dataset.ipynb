{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unidade03_cats-and-dogs_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcCHeYYyapv3"
      },
      "source": [
        "# check whether GPU is provided\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7ghBxO0SuPH"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O cats_and_dogs_filtered.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AwU0DC6eocm"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = 'cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('data/')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BxhK1wTiFBu"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "base_dir = 'data/cats_and_dogs_filtered/'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "# test_dir = os.path.join(base_dir, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8PP7lcrgGnq"
      },
      "source": [
        "**Building our network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FopSstCWeq0A"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFEiOv8Zgj7Y"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9cP0Je9glDf"
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1wziOiSgtiU"
      },
      "source": [
        "**Data preprocessing**\n",
        "\n",
        "Devemos converter os dados em tensores de ponto flutuante. Devemos seguir as seguintes etapas:\n",
        "\n",
        "Leia os arquivos de imagem.\n",
        "Decodifique em pixels RGB\n",
        "Converta-os em tensores de ponto flutuante.\n",
        "Rescale de cada pixels de (0 a 255) para [0, 1]\n",
        "\n",
        "O keras.preprocessing.image contém a classe ImageDataGenerator, que permite configurar geradores Python que podem transformar automaticamente arquivos de imagem em disco em lotes de tensores pré-processados. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXYUCmOcgnDY"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 150x150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8GgSWGQKf7r"
      },
      "source": [
        "Vamos dar uma olhada no formato dos arquivos gerados pelo python\n",
        "\n",
        "Foram produzidos batches de imagens RGB de 150x150 (shape (20, 150, 150, 3)) e labels binárias (shape (20,))\n",
        "\n",
        "20 é o número de amostras em cada batch (o tamanho do batch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wccSRTyNi9GJ"
      },
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "    print('data batch shape:', data_batch.shape)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob2YuvNVLPeX"
      },
      "source": [
        "**Treinando o modelo**\n",
        "\n",
        "Vamos ajustar nosso modelo aos dados usando o gerador, usando o método **fit_generator**\n",
        "\n",
        "train_generator: (primeiro argumento) produzirá batches indefinidamente. \n",
        "\n",
        "Como os dados estão sendo gerados infinitamente, o gerador precisa saber por exemplo quantas amostras extrair do gerador antes de declarar uma época.\n",
        "\n",
        "Assim, usamos o **steps_per_epoch** para que, depois de extrair \"steps_per_epoch\" batches do gerador, o model.fit irá para a próxima época. Neste caso, cada batch possui 20 amostras, portanto, serão necesssário 100 batches até atingirmos o objetivo de 2000 amostras.  \n",
        "\n",
        "**validation_data**: pode ser um gerador (como neste caso) e portanto, temos que fornecer o argumento **validation_steps** pois o gerador irá gerar dados infinitamente. O argumento validation_steps determina quantos batches serão extraídos do gerador Python de validação para a avaliação do modelo (50 pois nosso conjunto de validação possui 1000 amostras). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkRFxkHhjPZW"
      },
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiAlEpNHjw_I"
      },
      "source": [
        "model.save('cats_and_dogs_small_1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2aiwWLYj16K"
      },
      "source": [
        "Vamos plotar a loss e a acurácia do modelo sobre os dados de treinamento e validação durante o treinamento:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2zzKgwhj2ZR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FPGbsB1QUNq"
      },
      "source": [
        "Observe que a acurácia dos dados de treinamento aumenta linearmente ao longo do tempo até atingir quase 100% (e validação não passa de 70%).\n",
        "\n",
        "A validation loss atinge seu mínimo depois de apenas cinco épocas, e a train loss continua diminuindo linearmente até atingir quase 0.\n",
        "\n",
        "Como temos apenas poucas amostras de treinamento (2000), o maior risco é de overfitting (poderíamos resolver o problema com dropout). \n",
        "\n",
        "No entanto, usaremos uma técnica muito como para visão computacional que pode solucionar o problema de overfitting: **data augmentation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NHVVxcYRyQH"
      },
      "source": [
        "Mas, antes de abordarmos o assunto de data augmentation, vamos aprender a realizar algumas poredições no nosso modelo treinado.\n",
        "\n",
        "**Realizando Predições**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC1mo05bSDsc"
      },
      "source": [
        "Obtendo uma imagem de exemplo usando no treinamento (veja que esta imagem já está pré-processada)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbVcfetnnHgH"
      },
      "source": [
        "data_batch, labels_batch = validation_generator[0]\n",
        "print('data batch shape:', data_batch.shape)\n",
        "print(data_batch[5].shape)\n",
        "\n",
        "x = data_batch[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkM9QWryVWRp"
      },
      "source": [
        "Observe o shape da imagem (150, 150, 3), para a predição temos que converter para um tensor 4D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50YbCvezmiOO"
      },
      "source": [
        "x = x.reshape(1, 150, 150, 3)\n",
        "\n",
        "print (x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59XlvqiLoN2v"
      },
      "source": [
        "Realizando a inferência: a loss function binary_crossentropy retorna um valor de probabilidade entre 0 e 1 para a classificação binária (cat - 0 e dog - 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Eug5ibhoM7w"
      },
      "source": [
        "prob = model.predict(x)\n",
        "\n",
        "print(prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbdCqR8NuaWG"
      },
      "source": [
        "Abrindo uma imagem diretamente do diretório. Obsserve que agora a imagem não está pre-processada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqi5B3cwum0g"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "path = os.path.join(base_dir, 'validation/dogs/dog.2010.jpg')\n",
        "\n",
        "img = cv2.imread(path) \n",
        "cv2_imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdbUCJxTveOh"
      },
      "source": [
        "print(img.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erhEMHoiVtUc"
      },
      "source": [
        "Vamos carregar a imagem usando PIL (biblioteca para processamento de imagens em Python)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIBoU5wYwSN9"
      },
      "source": [
        "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "from PIL import Image\n",
        "\n",
        "img = load_img(path, target_size=(150, 150))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tmphD7UV2k_"
      },
      "source": [
        "Rescale de cada pixels de (0 a 255) para [0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFQObhcnCe83"
      },
      "source": [
        "from numpy import asarray\n",
        "\n",
        "img = asarray(img)\n",
        "\n",
        "print('Data Type: %s' % img.dtype)\n",
        "print('Min: %.3f, Max: %.3f' % (img.min(), img.max()))\n",
        "\n",
        "img = img.astype('float32')\n",
        "img /= 255.0\n",
        "\n",
        "print('Min: %.3f, Max: %.3f' % (img.min(), img.max()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOFMH9UYCyk7"
      },
      "source": [
        "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
        "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
        "\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDTtT8qUBU3t"
      },
      "source": [
        "prob = model.predict(x)\n",
        "\n",
        "print(prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDjAJjno_Y7S"
      },
      "source": [
        "**Usando data augmentation**\n",
        "\n",
        "O problema de overfitting foi causado por ter poucas amostras para aprender, ou seja, faz com que o modelo seja incapazes de generalizar para novos dados.\n",
        "\n",
        "Data augmentation gera mais dados de treinamento a partir de amostras existentes de treinamento, por meio de várias transformações aleatórias que produzem novas imagens. \n",
        "\n",
        "O objetivo é que, durante o treinamento, nosso modelo nunca veja exatamente a mesma imagem duas vezes. Isso ajuda o modelo a se expor a mais aspectos dos dados e a generalizar melhor.\n",
        "\n",
        "Você pude utilizar operações de processamento de imagens comuns para produzir novas amostras de dados a partir das imagens de treinamento.\n",
        "\n",
        "Acesse para exemplos:\n",
        "\n",
        "https://github.com/albumentations-team/albumentations\n",
        "\n",
        "https://github.com/codebox/image_augmentor\n",
        "\n",
        "\n",
        "No Keras, isso pode ser feito configurando várias transformações aleatórias a serem executadas nas imagens lidas pela nossa instância do ImageDataGenerator. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QODDjYgi_b95"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmfIYy6oEvpN"
      },
      "source": [
        "Estas são apenas algumas das opções disponíveis (para mais, consulte a documentação do Keras). \n",
        "\n",
        "rotation_range é um valor em graus (0-180), um intervalo no qual gira aleatoriamente as imagens.\n",
        "\n",
        "width_shift e height_shift são intervalos dentro dos quais rotaciona aleatoriamente imagens na vertical ou na horizontal.\n",
        "\n",
        "shear_range é para aplicar aleatoriamente transformações de cisalhamento.\n",
        "\n",
        "zoom_range é para ampliar aleatoriamente as imagens.\n",
        "\n",
        "horizontal_flip é para virar aleatoriamente metade das imagens horizontalmente\n",
        "\n",
        "fill_mode é a estratégia usada para preencher pixels recém-criados, que podem aparecer após uma rotação ou uma mudança de largura / altura."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgvb0ZwUFcau"
      },
      "source": [
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8yKbIT_E18q"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
        "\n",
        "# We pick one image to \"augment\"\n",
        "img_path = fnames[3]\n",
        "\n",
        "# Read the image and resize it\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "\n",
        "# Convert it to a Numpy array with shape (150, 150, 3)\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "# Reshape it to (1, 150, 150, 3)\n",
        "x = x.reshape((1,) + x.shape)\n",
        "\n",
        "# The .flow() command below generates batches of randomly transformed images.\n",
        "# It will loop indefinitely, so we need to `break` the loop at some point!\n",
        "i = 0\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "    plt.figure(i)\n",
        "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
        "    i += 1\n",
        "    if i % 4 == 0:\n",
        "        break\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aufXBAbDZqrW"
      },
      "source": [
        "Se treinarmos uma nova rede usando data augmentation, a rede nunca verá duas vezes a mesma entrada. No entanto, isso pode não ser suficiente para se livrar completamente do problema de overfitting. Para isso, também adicionaremos uma camada Dropout ao modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rmrQJ7uGYg7"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKVi74nAGeSO"
      },
      "source": [
        "Treinando o modelo usando data augmentation e dropout\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBpqpt3UGfDN"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,)\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 150x150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7RV09C_G2n5"
      },
      "source": [
        "model.save('cats_and_dogs_small_2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eilFzqctG4AE"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr7cGIIjaWZ_"
      },
      "source": [
        "Observe que agora não estamos mais enfrentando o problema de overfitting. \n",
        "\n",
        "As curvas de treinamento estão acompanhando de perto as curvas de validação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKAiEu9aaujk"
      },
      "source": [
        "Referência: François Chollet. Deep Learning with Python. November 2017  "
      ]
    }
  ]
}