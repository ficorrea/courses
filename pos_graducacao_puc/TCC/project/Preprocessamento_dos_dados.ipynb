{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessamento dos dados.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Tratando a base**\n"
      ],
      "metadata": {
        "id": "aH1eRITEeC5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dicionário de dados:\n",
        "#### Disponível em <https://www.gov.br/prf/pt-br/acesso-a-informacao/dados-abertos/dicionario-acidentes>\n",
        "* **id**: Variável com valores numéricos, representando o identificador do acidente;\n",
        "* **data_inversa**: Data da ocorrência no formato dd/mm/aaaa;\n",
        "* **dia_semana**: Dia da semana da ocorrência. Ex.: Segunda, Terça, etc;\n",
        "* **horario**: Horário da ocorrência no formato hh:mm:ss;\n",
        "* **uf**: Unidade da Federação. Ex.: MG, PE, DF, etc;\n",
        "* **br**: Variável com valores numéricos representando o identificador da BR do acidente;\n",
        "* **km**: Identificação do quilômetro onde ocorreu o acidente, com\n",
        "valor mínimo de 0,1 km e com a casa decimal separada\n",
        "por ponto;\n",
        "* **municipio**: Nome do município de ocorrência do acidente;\n",
        "* **causa_acidente**: Identificação da causa presumível do acidente. Ex.: Falta de atenção, Velocidade incompatível, etc;\n",
        "* **tipo_acidente**: Identificação do tipo de acidente. Ex.: Colisão frontal, Saída de pista,etc;\n",
        "* **classificacao_acidente**: Classificação quanto à gravidade do acidente: Sem Vítimas, Com Vítimas Feridas, Com Vítimas Fatais e Ignorado; \n",
        "* **fase_dia**: Fase do dia no momento do acidente. Ex. Amanhecer, Pleno dia, etc;\n",
        "* **sentido_via**: Sentido da via considerando o ponto de colisão: Crescente e decrescente;\n",
        "* **condicao_meteorologica**: Condição meteorológica no momento do acidente: Céu claro, chuva,vento,etc;\n",
        "* **tipo_pista**: Tipo da pista considerando a quantidade de faixas:Dupla,simples ou múltipla;\n",
        "* **tracado_via**: Descrição do traçado da via: reta, curva ou cruzamento;\n",
        "* **uso_solo**: Descrição sobre as características do local do acidente: Urbano=Sim ou Rural=Não;\n",
        "* **ano**: Ano da ocorrência;\n",
        "* **pessoas**: Total de pessoas envolvidas na ocorrência;\n",
        "* **mortos**: Total de pessoas mortas envolvidas na ocorrência;\n",
        "* **feridos_leves**: Total de pessoas com ferimentos leves envolvidas na ocorrência;\n",
        "* **feridos_graves**: Total de pessoas com ferimentos graves envolvidas na ocorrência;\n",
        "* **ilesos**: Total de pessoas ilesas envolvidas na ocorrência;\n",
        "* **ignorados**: Total de pessoas envolvidas na ocorrência e que não se soube o estado físico;\n",
        "* **feridos**: Total de pessoas feridas envolvidas na ocorrência (é a soma dos feridos leves com os graves);\n",
        "* **veiculos**: Total de veículos envolvidos na ocorrência;\n",
        "* **latitude**: Latitude do local do acidente em formato geodésico\n",
        "decimal;\n",
        "* **longitude**: Longitude do local do acidente em formato geodésico\n",
        "decimal;\n",
        "* **regional**: - \n",
        "* **delegacia**: -\n",
        "* **uop**: -"
      ],
      "metadata": {
        "id": "9-yz_8LLfyOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math as mt\n",
        "from unicodedata import normalize"
      ],
      "metadata": {
        "id": "Jigrhet2eJuI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets concatenation\n",
        "\n",
        "datasets = [f'/content/drive/MyDrive/PosPuc/TCC/datasets/acidentes_ocorrencia/datatran{i}.csv' for i in range(2007, 2022)]\n",
        "df_geral = pd.DataFrame()\n",
        "\n",
        "for data in datasets:\n",
        "    df_temp = pd.read_csv(data, sep=';', encoding='utf-8')\n",
        "    df_geral = pd.concat([df_geral, df_temp], ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5yxTsR-rM-T",
        "outputId": "5337aa7a-a520-4d4c-868e-0ca334e28599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy dataframe\n",
        "df = df_geral.copy()"
      ],
      "metadata": {
        "id": "0xRJvUkb5Mlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset shape\n",
        "df.shape"
      ],
      "metadata": {
        "id": "S2O0IHgk0LpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset check columns types\n",
        "df.info()"
      ],
      "metadata": {
        "id": "On0XHb5WAaCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using missingno package to check dataset health\n",
        "import missingno as msno\n",
        "msno.matrix(df)"
      ],
      "metadata": {
        "id": "xIRTWcr77v5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset head / tail\n",
        "df.head()"
      ],
      "metadata": {
        "id": "6T2HBQNF9Bvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "Ivo1vEYj9Pbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check ID duplicity\n",
        "id_duplicity = df.groupby('id', as_index=False).agg({'id': ['first', 'count']})\n",
        "ids_duplicity = id_duplicity[id_duplicity[('id', 'count')] > 1][('id', 'first')].values.tolist()\n",
        "len(ids_duplicity)"
      ],
      "metadata": {
        "id": "D83gW2Bv9nzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.id == ids_duplicity[0]]"
      ],
      "metadata": {
        "id": "85POTrJYEWza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns NaN\n",
        "df[(df.br == '(null)') | (df.br.isna())]"
      ],
      "metadata": {
        "id": "gZlpRy2n_Nlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusions**\n",
        "* Must be necessary:\n",
        "    * Convert feature **id** from float to str;\n",
        "    * Fill features **br**, **km**, **regional**, **delegacia** and **uop**;\n",
        "    * Convert feature **br** from float to str;\n",
        "    * Truncate feature **km**;\n",
        "    * Create a interval of 50km from feature **km**, eg: if km = 67 the interval must be 50 - 100;\n",
        "    * Adjust feature **data_inversa** to format aaaa-mm-dd;\n",
        "    * Adjust feature **dia_semana** to only first name and capitalize, eg: SABADO, SEGUNDA, etc;\n",
        "    * Remove accentuation from features **dia_semana**, **causa_acidente**, **tipo_acidente**, **classificacao_acidente**, **fase_dia**, **condicao_metereologica**, **tipo_pista**, **tracado_via** e **uso_solo**;\n",
        "    * Capitalize features **dia_semana**, **causa_acidente**, **tipo_acidente**, **classificacao_acidente**, **fase_dia**, **sentido_via**, **condicao_metereologica**, **tipo_pista**, **tracado_via** e **uso_solo**;\n",
        "    * Fill feature **ano** with respective year where is null and convert to int;\n",
        "    * Check duplicity into **id** column, some duplications came from same id but different years;\n",
        "    * Features **latitude**, **longitude** are written with ' , ' change to ' . ' and convert to float;\n",
        "    * Check features **latitude**, **longitude** and fill nan values;\n",
        "    * Add **mes** field;\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wXXkPM8cwrG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert features id\n",
        "df['id'] = df.id.astype(int).astype(str)"
      ],
      "metadata": {
        "id": "mEin45qB1SMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fillna features br, km, regional, delegacia and uop\n",
        "values = {'br': '0', 'km': '-1', 'regional': 'UNKNOWN', 'delegacia': 'UNKNOWN', 'uop': 'UNKNOWN'}\n",
        "df.fillna(values, inplace=True)"
      ],
      "metadata": {
        "id": "tEgmR4q9_4Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert features br and km\n",
        "df.loc[df.br == '(null)', 'br'] = '0'\n",
        "df['br'] = df.br.astype(int).astype(str)\n",
        "\n",
        "df.loc[df.km == '(null)', 'km'] = '-1'\n",
        "df['km'] = df.apply(lambda x: x.km.replace(',', '.') if isinstance(x.km, str) else x.km, axis=1)\n",
        "df['km'] = df.km.astype(float).astype(int)"
      ],
      "metadata": {
        "id": "VYWkTeypArKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a km_interval\n",
        "def gen_interval(x, num_interval):\n",
        "    int_min = x - x % num_interval\n",
        "    int_max = int_min + num_interval\n",
        "    return f'[{int_min} - {int_max}]'\n",
        "\n",
        "df['km_intervalo'] = df.apply(lambda x: gen_interval(x.km, 50), axis=1)"
      ],
      "metadata": {
        "id": "z1m6mHRtcTqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust feature data_inversa to format aaaa-mm-dd\n",
        "df['data_inversa'] = pd.to_datetime(df.data_inversa, dayfirst=True)"
      ],
      "metadata": {
        "id": "VOnpNWVOMrLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust feature dia_semana to only first name and capitalize, eg: SABADO, SEGUNDA, etc\n",
        "df['dia_semana'] = df.apply(lambda x: x.dia_semana.split('-')[0].upper(), axis=1)"
      ],
      "metadata": {
        "id": "UlXr58FgfDJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove nan values\n",
        "fill_value = '(null)'\n",
        "\n",
        "df.fillna({'dia_semana': fill_value,\n",
        "           'causa_acidente': fill_value, \n",
        "           'tipo_acidente':fill_value, \n",
        "           'classificacao_acidente': fill_value, \n",
        "           'fase_dia': fill_value, \n",
        "           'condicao_metereologica': fill_value, \n",
        "           'tipo_pista': fill_value, \n",
        "           'tracado_via': fill_value, \n",
        "           'uso_solo': fill_value}, inplace=True)\n",
        "\n",
        "# Remove accentuation and capitalize strings\n",
        "def remove_accents(x):\n",
        "    return normalize('NFKD', x).encode('ASCII', 'ignore').decode('ASCII').upper()\n",
        "\n",
        "df = (\n",
        "    df\n",
        "    .assign(dia_semana = df.apply(lambda x: remove_accents(x.dia_semana), axis=1))\n",
        "    .assign(causa_acidente = df.apply(lambda x: remove_accents(x.causa_acidente), axis=1))\n",
        "    .assign(tipo_acidente = df.apply(lambda x: remove_accents(x.tipo_acidente), axis=1))\n",
        "    .assign(classificacao_acidente = df.apply(lambda x: remove_accents(x.classificacao_acidente), axis=1))\n",
        "    .assign(fase_dia = df.apply(lambda x: remove_accents(x.fase_dia), axis=1))\n",
        "    .assign(condicao_metereologica = df.apply(lambda x: remove_accents(x.condicao_metereologica), axis=1))\n",
        "    .assign(tipo_pista = df.apply(lambda x: remove_accents(x.tipo_pista), axis=1))\n",
        "    .assign(tracado_via = df.apply(lambda x: remove_accents(x.tracado_via), axis=1))\n",
        "    .assign(uso_solo = df.apply(lambda x: remove_accents(x.uso_solo), axis=1))\n",
        ")"
      ],
      "metadata": {
        "id": "OEzzqo9HdUuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill feature ano\n",
        "df['ano'] = df.apply(lambda x: x.data_inversa.year, axis=1)"
      ],
      "metadata": {
        "id": "EvBUYwrywRq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check ID duplicity\n",
        "id_duplicity = df.groupby(['id', 'ano'], as_index=False).agg({'id': ['first', 'count'], 'ano': 'first'})\n",
        "ids_duplicity = id_duplicity[id_duplicity[('id', 'count')] > 1][('id', 'first')].values.tolist()\n",
        "print(f' Was found: {len(ids_duplicity)} duplicate ids')\n",
        "\n",
        "# Remove duplicity\n",
        "df.drop_duplicates(subset=['id', 'ano'], keep='first', inplace=True)"
      ],
      "metadata": {
        "id": "OydqZGAfyHdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change longitude and latitude fields type\n",
        "df['longitude'] = df.apply(lambda x: x.longitude if isinstance(x.longitude, float) else x.longitude.replace(',', '.'), axis=1)\n",
        "df['latitude'] = df.apply(lambda x: x.latitude if isinstance(x.latitude, float) else x.latitude.replace(',', '.'), axis=1)\n",
        "\n",
        "df['longitude'] = df['longitude'].astype(float)\n",
        "df['latitude'] = df['latitude'].astype(float)"
      ],
      "metadata": {
        "id": "EGiMSQgoqfnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill latitude and longitude\n",
        "df_temp = (\n",
        "    df\n",
        "    .groupby(['br', 'km_intervalo'], as_index=False)\n",
        "    .agg({'longitude': 'mean', 'latitude': 'mean'})\n",
        ").rename(columns={'longitude': 'temp_long', 'latitude': 'temp_lati'}, inplace=True)\n",
        "\n",
        "# Joining tables\n",
        "df = df.merge(df_temp, on=['br', 'km_intervalo'], how='left')\n",
        "\n",
        "df = (\n",
        "    df\n",
        "    .assing(longitude = df.apply(lambda x: x.temp_long if (np.isnan(x.longitude) and not np.isnan(x.temp_long)) else 0, axis=1))\n",
        "    .assing(latitude = df.apply(lambda x: x.temp_lati if (np.isnan(x.latitude) and not np.isnan(x.temp_lati)) else 0, axis=1))\n",
        ")\n",
        "\n",
        "# Remove unnecessary columns\n",
        "df.drop(columns=['temp_long', 'temp_lati'], inplace=True)"
      ],
      "metadata": {
        "id": "txdo6b9n3yD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add feature mes\n",
        "df['mes'] = df.apply(lambda x: x.data_inversa.month, axis=1)"
      ],
      "metadata": {
        "id": "RrPw4zqYfAyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final dataset health\n",
        "import missingno as msno\n",
        "msno.matrix(df)"
      ],
      "metadata": {
        "id": "FxfMHJcFoUps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save dataframe preprocessed\n",
        "df.to_csv('/content/drive/MyDrive/PosPuc/TCC/datasets/acidentes_ocorrencia/df_preprocessed_new.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "GofztcH9sZkM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}